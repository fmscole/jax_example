{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial   \n",
    "from typing import Any, Callable, Mapping, Optional, Sequence, Tuple, Union\n",
    "from flax import linen as nn\n",
    "from flax.linen.activation import sigmoid\n",
    "from flax.linen.activation import tanh\n",
    "from flax.linen.initializers import zeros\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from flax.linen.initializers import orthogonal\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "PRNGKey = Any\n",
    "Shape = Tuple[int]\n",
    "Dtype = Any  # this could be a real type?\n",
    "Array = Any\n",
    "\n",
    "class LSTMCell(nn.Module):  \n",
    "  gate_fn: Callable[..., Any] = sigmoid\n",
    "  activation_fn: Callable[..., Any] = tanh\n",
    "  kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init\n",
    "  recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = orthogonal()\n",
    "  bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = zeros\n",
    "  dtype: Optional[Dtype] = None\n",
    "  param_dtype: Dtype = jnp.float32\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, carry, inputs):\n",
    "    c, h = carry\n",
    "    hidden_features = h.shape[-1]\n",
    "    dense_h = partial(nn.Dense,\n",
    "                      features=hidden_features,\n",
    "                      use_bias=True,\n",
    "                      kernel_init=self.recurrent_kernel_init,\n",
    "                      bias_init=self.bias_init,\n",
    "                      dtype=self.dtype,\n",
    "                      param_dtype=self.param_dtype)\n",
    "    dense_i = partial(nn.Dense,\n",
    "                      features=hidden_features,\n",
    "                      use_bias=False,\n",
    "                      kernel_init=self.kernel_init,\n",
    "                      dtype=self.dtype,\n",
    "                      param_dtype=self.param_dtype)\n",
    "    i = self.gate_fn(dense_i(name='ii')(inputs) + dense_h(name='hi')(h))\n",
    "    f = self.gate_fn(dense_i(name='if')(inputs) + dense_h(name='hf')(h))\n",
    "    g = self.activation_fn(dense_i(name='ig')(inputs) + dense_h(name='hg')(h))\n",
    "    o = self.gate_fn(dense_i(name='io')(inputs) + dense_h(name='ho')(h))\n",
    "    new_c = f * c + i * g\n",
    "    new_h = o * self.activation_fn(new_c)\n",
    "    return (new_c, new_h), new_h\n",
    "\n",
    "  @staticmethod\n",
    "  def initialize_carry(rng, batch_dims, size, init_fn=zeros):\n",
    "    key1, key2 = jax.random.split(rng)\n",
    "    mem_shape = batch_dims + (size,)\n",
    "    return init_fn(key1, mem_shape), init_fn(key2, mem_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleScan(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, xs,is_training:bool=True):\n",
    "        LSTM = nn.scan(LSTMCell,\n",
    "                    variable_broadcast=\"params\",\n",
    "                    split_rngs={\"params\": False},\n",
    "                    in_axes=1,\n",
    "                    out_axes=1)            \n",
    "       \n",
    "        ch = LSTMCell.initialize_carry(jax.random.PRNGKey(0), (xs.shape[0],), 100)\n",
    "        ch, xs=LSTM()(ch, xs)                \n",
    "        # xs=nn.Dropout(rate=0.2,deterministic=not is_training)(xs)\n",
    "\n",
    "        k=jnp.einsum(\"btv,bvs->bts\",xs,xs.transpose(0,2,1))\n",
    "        k=jax.nn.softmax(k/xs.shape[-1]**0.5,axis=1)\n",
    "        xs=jnp.einsum(\"btv,bvs->bts\",k,xs)\n",
    "\n",
    "        ch = LSTMCell.initialize_carry(jax.random.PRNGKey(0), (xs.shape[0],), 200)\n",
    "        ch, xs=LSTM()(ch, xs)  \n",
    "        # xs=nn.Dropout(rate=0.2,deterministic=not is_training)(xs)\n",
    "\n",
    "        k=jnp.einsum(\"btv,bvs->bts\",xs,xs.transpose(0,2,1))\n",
    "        k=jax.nn.softmax(k/xs.shape[-1]**0.5,axis=1)\n",
    "        xs=jnp.einsum(\"btv,bvs->bts\",k,xs)\n",
    "\n",
    "        ch = LSTMCell.initialize_carry(jax.random.PRNGKey(0), (xs.shape[0],), 10)\n",
    "        ch, xs=LSTM()(ch, xs)        \n",
    "        # xs=nn.Dropout(rate=0.2,deterministic=not is_training)(xs)\n",
    "        \n",
    "        # ch = LSTMCell.initialize_carry(jax.random.PRNGKey(0), (xs.shape[0],), 200)\n",
    "        # ch, xs=LSTM()(ch, xs)\n",
    "        # xs=nn.Dropout(rate=0.2,deterministic=not is_training)(xs)\n",
    "        \n",
    "        xs=xs[:,-1,:]\n",
    "        # xs=nn.Dense(features=10)(xs)        \n",
    "        return xs\n",
    "\n",
    "lstm = SimpleScan()\n",
    "key_1, key_2, key_3 = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "variables=lstm.init({'params': key_1, 'dropout':key_1}, jnp.ones([100, 28, 28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(state, images, labels,dropout_rng):\n",
    "  def loss_fn(params,labels):\n",
    "    logits= state.apply_fn({'params': params}, images,is_training=True,rngs={'dropout':dropout_rng})\n",
    "    \n",
    "    labels=jax.nn.one_hot(labels,10)\n",
    "    loss = jnp.mean(jnp.sum(0.5*(logits-labels)**2,axis=-1))   \n",
    "    \n",
    "    return loss, (logits)    \n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, (logits)), grads = grad_fn(state.params,labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)  \n",
    "  return grads, loss, accuracy,logits\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "  return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng):\n",
    "  lstm = SimpleScan()\n",
    "  key_1, key_2, key_3 = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "  variables=lstm.init({'params': key_1, 'dropout':key_1}, jnp.ones([1, 28, 28]))\n",
    "  params = variables['params']\n",
    "  \n",
    "  import optax\n",
    "  from flax.training import train_state\n",
    "  \n",
    "  tx = optax.adam(learning_rate=0.01)\n",
    "  state=train_state.TrainState.create(apply_fn=lstm.apply, params=params, tx=tx)\n",
    "  return state,variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(state, variables,image_i):  \n",
    "  logits= state.apply_fn({'params': state.params},image_i,is_training=False)\n",
    "  return logits\n",
    "def test(state, batch_stats,test_ds):\n",
    "  images = test_ds['image']\n",
    "  labels = test_ds['label']\n",
    "  images=images.reshape(-1,28,28)\n",
    "  batchs=1000\n",
    "  accuracy=0\n",
    "  for i in range(0,len(images),batchs):\n",
    "    image_i=images[i:i+batchs]\n",
    "    label_i=labels[i:i+batchs]\n",
    "    logits= predict(state, batch_stats,image_i)\n",
    "    accuracy += jnp.sum(jnp.argmax(logits, -1) == label_i)  \n",
    "  return accuracy/len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds,  rng):\n",
    "  batch_size=1000\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "  epoch_loss = []\n",
    "  epoch_accuracy = []\n",
    "  for perm in perms:\n",
    "    batch_images = train_ds['image'][perm, ...]\n",
    "    batch_labels = train_ds['label'][perm, ...]\n",
    "    batch_images=batch_images.reshape(-1,28,28)\n",
    "    dropout_rng,rng=jax.random.split(rng)\n",
    "    grads, loss, accuracy,y = apply_model(state, batch_images, batch_labels,dropout_rng=dropout_rng)\n",
    "    \n",
    "    state = update_model(state, grads)\n",
    "    \n",
    "    epoch_loss.append(loss)\n",
    "    epoch_accuracy.append(accuracy)\n",
    "  train_loss = np.mean(epoch_loss)\n",
    "  train_accuracy = np.mean(epoch_accuracy)\n",
    "  return state, train_loss, train_accuracy,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf \n",
    "def get_datasets():\n",
    "  with tf.device('/cpu:0'):\n",
    "    ds_builder = tfds.builder('fashion_mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():  \n",
    "  train_ds, test_ds = get_datasets()\n",
    "  rng = jax.random.PRNGKey(0)  \n",
    "  rng, init_rng = jax.random.split(rng)\n",
    "  state,variables = create_train_state(init_rng)\n",
    "  for epoch in range(1, 100 + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, train_loss, train_accuracy,y = train_epoch(state, train_ds,\n",
    "                                                    rng=input_rng)   \n",
    "    print(\"\") \n",
    "    print(\"epoch:\",epoch)                                         \n",
    "    print(f\"train Error_ratio={train_accuracy:2.4f}\")     \n",
    "    print(f\"test Error_ratio={test(state, variables,test_ds):2.4f}\")\n",
    "    # print(y)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fms/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:622: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.get_single_element()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1\n",
      "train Error_ratio=0.6364\n",
      "test Error_ratio=0.7418\n",
      "\n",
      "epoch: 2\n",
      "train Error_ratio=0.8011\n",
      "test Error_ratio=0.8036\n",
      "\n",
      "epoch: 3\n",
      "train Error_ratio=0.8353\n",
      "test Error_ratio=0.8385\n",
      "\n",
      "epoch: 4\n",
      "train Error_ratio=0.8490\n",
      "test Error_ratio=0.8444\n",
      "\n",
      "epoch: 5\n",
      "train Error_ratio=0.8623\n",
      "test Error_ratio=0.8571\n",
      "\n",
      "epoch: 6\n",
      "train Error_ratio=0.8714\n",
      "test Error_ratio=0.8651\n",
      "\n",
      "epoch: 7\n",
      "train Error_ratio=0.8747\n",
      "test Error_ratio=0.8632\n",
      "\n",
      "epoch: 8\n",
      "train Error_ratio=0.8808\n",
      "test Error_ratio=0.8702\n",
      "\n",
      "epoch: 9\n",
      "train Error_ratio=0.8882\n",
      "test Error_ratio=0.8709\n",
      "\n",
      "epoch: 10\n",
      "train Error_ratio=0.8917\n",
      "test Error_ratio=0.8751\n",
      "\n",
      "epoch: 11\n",
      "train Error_ratio=0.8938\n",
      "test Error_ratio=0.8730\n",
      "\n",
      "epoch: 12\n",
      "train Error_ratio=0.8990\n",
      "test Error_ratio=0.8836\n",
      "\n",
      "epoch: 13\n",
      "train Error_ratio=0.9031\n",
      "test Error_ratio=0.8842\n",
      "\n",
      "epoch: 14\n",
      "train Error_ratio=0.9042\n",
      "test Error_ratio=0.8826\n",
      "\n",
      "epoch: 15\n",
      "train Error_ratio=0.9069\n",
      "test Error_ratio=0.8898\n",
      "\n",
      "epoch: 16\n",
      "train Error_ratio=0.9088\n",
      "test Error_ratio=0.8892\n",
      "\n",
      "epoch: 17\n",
      "train Error_ratio=0.9110\n",
      "test Error_ratio=0.8909\n",
      "\n",
      "epoch: 18\n",
      "train Error_ratio=0.9121\n",
      "test Error_ratio=0.8889\n",
      "\n",
      "epoch: 19\n",
      "train Error_ratio=0.9153\n",
      "test Error_ratio=0.8840\n",
      "\n",
      "epoch: 20\n",
      "train Error_ratio=0.9176\n",
      "test Error_ratio=0.8935\n",
      "\n",
      "epoch: 21\n",
      "train Error_ratio=0.9203\n",
      "test Error_ratio=0.8961\n",
      "\n",
      "epoch: 22\n",
      "train Error_ratio=0.9209\n",
      "test Error_ratio=0.8943\n",
      "\n",
      "epoch: 23\n",
      "train Error_ratio=0.9242\n",
      "test Error_ratio=0.8956\n",
      "\n",
      "epoch: 24\n",
      "train Error_ratio=0.9236\n",
      "test Error_ratio=0.8933\n",
      "\n",
      "epoch: 25\n",
      "train Error_ratio=0.9251\n",
      "test Error_ratio=0.8881\n",
      "\n",
      "epoch: 26\n",
      "train Error_ratio=0.9201\n",
      "test Error_ratio=0.8969\n",
      "\n",
      "epoch: 27\n",
      "train Error_ratio=0.9261\n",
      "test Error_ratio=0.9002\n",
      "\n",
      "epoch: 28\n",
      "train Error_ratio=0.9284\n",
      "test Error_ratio=0.8958\n",
      "\n",
      "epoch: 29\n",
      "train Error_ratio=0.9298\n",
      "test Error_ratio=0.8965\n",
      "\n",
      "epoch: 30\n",
      "train Error_ratio=0.9331\n",
      "test Error_ratio=0.8998\n",
      "\n",
      "epoch: 31\n",
      "train Error_ratio=0.9346\n",
      "test Error_ratio=0.8941\n",
      "\n",
      "epoch: 32\n",
      "train Error_ratio=0.9341\n",
      "test Error_ratio=0.8969\n",
      "\n",
      "epoch: 33\n",
      "train Error_ratio=0.9388\n",
      "test Error_ratio=0.8978\n",
      "\n",
      "epoch: 34\n",
      "train Error_ratio=0.9378\n",
      "test Error_ratio=0.9009\n",
      "\n",
      "epoch: 35\n",
      "train Error_ratio=0.9375\n",
      "test Error_ratio=0.8963\n",
      "\n",
      "epoch: 36\n",
      "train Error_ratio=0.9374\n",
      "test Error_ratio=0.8991\n",
      "\n",
      "epoch: 37\n",
      "train Error_ratio=0.9409\n",
      "test Error_ratio=0.9019\n",
      "\n",
      "epoch: 38\n",
      "train Error_ratio=0.9464\n",
      "test Error_ratio=0.8983\n",
      "\n",
      "epoch: 39\n",
      "train Error_ratio=0.9435\n",
      "test Error_ratio=0.9009\n",
      "\n",
      "epoch: 40\n",
      "train Error_ratio=0.9474\n",
      "test Error_ratio=0.8982\n",
      "\n",
      "epoch: 41\n",
      "train Error_ratio=0.9472\n",
      "test Error_ratio=0.9019\n",
      "\n",
      "epoch: 42\n",
      "train Error_ratio=0.9472\n",
      "test Error_ratio=0.9021\n",
      "\n",
      "epoch: 43\n",
      "train Error_ratio=0.9468\n",
      "test Error_ratio=0.8969\n",
      "\n",
      "epoch: 44\n",
      "train Error_ratio=0.9441\n",
      "test Error_ratio=0.8939\n",
      "\n",
      "epoch: 45\n",
      "train Error_ratio=0.9446\n",
      "test Error_ratio=0.9006\n",
      "\n",
      "epoch: 46\n",
      "train Error_ratio=0.9500\n",
      "test Error_ratio=0.9009\n",
      "\n",
      "epoch: 47\n",
      "train Error_ratio=0.9519\n",
      "test Error_ratio=0.8994\n",
      "\n",
      "epoch: 48\n",
      "train Error_ratio=0.9519\n",
      "test Error_ratio=0.8973\n",
      "\n",
      "epoch: 49\n",
      "train Error_ratio=0.9521\n",
      "test Error_ratio=0.9015\n",
      "\n",
      "epoch: 50\n",
      "train Error_ratio=0.9528\n",
      "test Error_ratio=0.8999\n",
      "\n",
      "epoch: 51\n",
      "train Error_ratio=0.9555\n",
      "test Error_ratio=0.9020\n",
      "\n",
      "epoch: 52\n",
      "train Error_ratio=0.9549\n",
      "test Error_ratio=0.8976\n",
      "\n",
      "epoch: 53\n",
      "train Error_ratio=0.9551\n",
      "test Error_ratio=0.8991\n",
      "\n",
      "epoch: 54\n",
      "train Error_ratio=0.9548\n",
      "test Error_ratio=0.9025\n",
      "\n",
      "epoch: 55\n",
      "train Error_ratio=0.9555\n",
      "test Error_ratio=0.9017\n",
      "\n",
      "epoch: 56\n",
      "train Error_ratio=0.9530\n",
      "test Error_ratio=0.8946\n",
      "\n",
      "epoch: 57\n",
      "train Error_ratio=0.9453\n",
      "test Error_ratio=0.9005\n",
      "\n",
      "epoch: 58\n",
      "train Error_ratio=0.9562\n",
      "test Error_ratio=0.9015\n",
      "\n",
      "epoch: 59\n",
      "train Error_ratio=0.9557\n",
      "test Error_ratio=0.8949\n",
      "\n",
      "epoch: 60\n",
      "train Error_ratio=0.9588\n",
      "test Error_ratio=0.8969\n",
      "\n",
      "epoch: 61\n",
      "train Error_ratio=0.9586\n",
      "test Error_ratio=0.9002\n",
      "\n",
      "epoch: 62\n",
      "train Error_ratio=0.9595\n",
      "test Error_ratio=0.8753\n",
      "\n",
      "epoch: 63\n",
      "train Error_ratio=0.9365\n",
      "test Error_ratio=0.8965\n",
      "\n",
      "epoch: 64\n",
      "train Error_ratio=0.9336\n",
      "test Error_ratio=0.8894\n",
      "\n",
      "epoch: 65\n",
      "train Error_ratio=0.9359\n",
      "test Error_ratio=0.8961\n",
      "\n",
      "epoch: 66\n",
      "train Error_ratio=0.9516\n",
      "test Error_ratio=0.9015\n",
      "\n",
      "epoch: 67\n",
      "train Error_ratio=0.9577\n",
      "test Error_ratio=0.8989\n",
      "\n",
      "epoch: 68\n",
      "train Error_ratio=0.9500\n",
      "test Error_ratio=0.8512\n",
      "\n",
      "epoch: 69\n",
      "train Error_ratio=0.8884\n",
      "test Error_ratio=0.8816\n",
      "\n",
      "epoch: 70\n",
      "train Error_ratio=0.9194\n",
      "test Error_ratio=0.8922\n",
      "\n",
      "epoch: 71\n",
      "train Error_ratio=0.9329\n",
      "test Error_ratio=0.8916\n",
      "\n",
      "epoch: 72\n",
      "train Error_ratio=0.9413\n",
      "test Error_ratio=0.8956\n",
      "\n",
      "epoch: 73\n",
      "train Error_ratio=0.9476\n",
      "test Error_ratio=0.8996\n",
      "\n",
      "epoch: 74\n",
      "train Error_ratio=0.9531\n",
      "test Error_ratio=0.8959\n",
      "\n",
      "epoch: 75\n",
      "train Error_ratio=0.9532\n",
      "test Error_ratio=0.8973\n",
      "\n",
      "epoch: 76\n",
      "train Error_ratio=0.9585\n",
      "test Error_ratio=0.8983\n",
      "\n",
      "epoch: 77\n",
      "train Error_ratio=0.9601\n",
      "test Error_ratio=0.8970\n",
      "\n",
      "epoch: 78\n",
      "train Error_ratio=0.9612\n",
      "test Error_ratio=0.8970\n",
      "\n",
      "epoch: 79\n",
      "train Error_ratio=0.9632\n",
      "test Error_ratio=0.8949\n",
      "\n",
      "epoch: 80\n",
      "train Error_ratio=0.9613\n",
      "test Error_ratio=0.9017\n",
      "\n",
      "epoch: 81\n",
      "train Error_ratio=0.9620\n",
      "test Error_ratio=0.8951\n",
      "\n",
      "epoch: 82\n",
      "train Error_ratio=0.9624\n",
      "test Error_ratio=0.8953\n",
      "\n",
      "epoch: 83\n",
      "train Error_ratio=0.9601\n",
      "test Error_ratio=0.8989\n",
      "\n",
      "epoch: 84\n",
      "train Error_ratio=0.9634\n",
      "test Error_ratio=0.9002\n",
      "\n",
      "epoch: 85\n",
      "train Error_ratio=0.9629\n",
      "test Error_ratio=0.8935\n",
      "\n",
      "epoch: 86\n",
      "train Error_ratio=0.9609\n",
      "test Error_ratio=0.8974\n",
      "\n",
      "epoch: 87\n",
      "train Error_ratio=0.9658\n",
      "test Error_ratio=0.8977\n",
      "\n",
      "epoch: 88\n",
      "train Error_ratio=0.9624\n",
      "test Error_ratio=0.8950\n",
      "\n",
      "epoch: 89\n",
      "train Error_ratio=0.9629\n",
      "test Error_ratio=0.8950\n",
      "\n",
      "epoch: 90\n",
      "train Error_ratio=0.9675\n",
      "test Error_ratio=0.8992\n",
      "\n",
      "epoch: 91\n",
      "train Error_ratio=0.9686\n",
      "test Error_ratio=0.9011\n",
      "\n",
      "epoch: 92\n",
      "train Error_ratio=0.9659\n",
      "test Error_ratio=0.8960\n",
      "\n",
      "epoch: 93\n",
      "train Error_ratio=0.9657\n",
      "test Error_ratio=0.8981\n",
      "\n",
      "epoch: 94\n",
      "train Error_ratio=0.9620\n",
      "test Error_ratio=0.8955\n",
      "\n",
      "epoch: 95\n",
      "train Error_ratio=0.9675\n",
      "test Error_ratio=0.8973\n",
      "\n",
      "epoch: 96\n",
      "train Error_ratio=0.9662\n",
      "test Error_ratio=0.8994\n",
      "\n",
      "epoch: 97\n",
      "train Error_ratio=0.9699\n",
      "test Error_ratio=0.8972\n",
      "\n",
      "epoch: 98\n",
      "train Error_ratio=0.9722\n",
      "test Error_ratio=0.8998\n",
      "\n",
      "epoch: 99\n",
      "train Error_ratio=0.9704\n",
      "test Error_ratio=0.8987\n",
      "\n",
      "epoch: 100\n",
      "train Error_ratio=0.9720\n",
      "test Error_ratio=0.9001\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414339477dce8ad21bdf41cacf05011be0c0809da68b1d12623186fed67fb677"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
